\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{array}
\usepackage{graphicx,amsmath,amsfonts,amssymb, epsfig,color,url, amsthm}
\usepackage{siunitx}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{url}
\usepackage{longtable}
\usepackage{csvsimple}
\usepackage{placeins} % put this in your pre-amble
\usepackage{flafter}
\usepackage{mathrsfs} % https://www.ctan.org/pkg/mathrsfs
\usepackage[version=4]{mhchem}
\usepackage{bm}
\usepackage{natbib}
\usepackage{mathtools}
\sisetup{detect-all}

\makeatletter
\providecommand\add@text{}
\newcommand\tagaddtext[1]{%
  \gdef\add@text{#1\gdef\add@text{}}}% 
\renewcommand\tagform@[1]{%  
  \maketag@@@{\llap{\add@text\quad}(\ignorespaces#1\unskip\@@italiccorr)}%
}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\B}[1]{\boldsymbol{#1}}

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\pagestyle{fancy}
\fancyhf{}
\lhead{Path Stability, Jeffrey Li}
\rfoot{Page \thepage}
\linespread{1}
\title{
  {\LARGE
   \textbf{Stability Of Paths}
   }\\~\\
   {\large
    \textbf{In what conditions is a path through a dynamic system stable?}
   }\\~\\
  {\large Mathematics Extended Essay}\\
  {\large Colonel By Secondary School} \\
  {\large Word Count: 3666}
}
\author{Jeffrey Li}
\date{\today}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[definition]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\doublespacing

\begin{document}

\maketitle
\break

\section*{Abstract}
In this paper, we first reestablish the foundations of classical
Lyapunov point stability. We then extend Lyapunov's arguments
and establish a rigorous definition of Path Stability, in line 
with Lyapunov's original reasoning. Next, we note some important 
properties derived from the definition of Path Stability which 
allow the emergent behavior of the path to be decomposed and 
analyzed individually on each point. Finally, by linearizing
the differential equation describing change of state, we arrive at insights
linking the stability of the path to the Quadratic Form of
the Jacobian of the system, another problem that has extensive 
literature in the field of linear algebra, demonstrating
the concept of Path Stability has deep conceptual roots
reaching into the heart of mathematics.

\break

\tableofcontents
\break


\section{Research Question}
Given a particle travelling under a dynamic system, 
under what conditions will an arbitrarily small disturbance to the initial 
position always result in an arbitrarily small displacement to the new path,
regardless of the time? 

\section{Introduction}
In life, we can often describe the phenomenon we observe as a product of two factors:
ever-present change and an element of randomness.

Firstly, the way in which things change often can be described as a dynamic system,
which are systems whose development in time is dependent upon their current state. Dynamic systems are integral to 
many fields of study; through Hamiltonian mechanics, we can understand motion
as a dynamic system whose state is defined as the combination of position and velocity.
In ecological models, populations will grow if resources are abundant, and shrink if
resources are scarce. Hence, the change from one year to the next is largely dependent
upon the population of a species from the year before. 

Secondly, no idealized model is ever perfectly accurate to its real world counterpart
because of the prevalence of random disturbances of various magnitudes throughout
every aspect of life. The work of the Russian mathematician
Aleksandr Lyapunov at the turn of the 20th century has given mathematicians 
frameworks and methods for understanding the stability of a system; 
in other words, to what degree the system is  
resilient to small initial disturbances. In the following century, this
topic has been extensively studied with respect to stable \textit{points},
but in comparison, I found far fewer works that deal with the
stability of entire \textit{paths}: studying how the state of a system
develops over an arbitrarily large duration of time.

\section{Motivation}

For me, the consideration of the problem of stability arose from
contemplating a ball in a rounded crater. The ball may start at any
point along within the semi-circular cavity, but would always roll
down and come to rest at the center. A ball at the top of the hill, however
would roll in very different directions when given two slightly different initial positions.
Similar concepts of stability can be found in prices, which are sometimes subject to extreme 
fluctuation and other times are extremely inert, or celestial motion, which was
the origin of Chaos Theory through the three-body problem, while at the same time 
illustrating unexpected stability in the form of Lagrange Points within an orbit. 
All of these led me to wonder if Stability could somehow be quantified, and be described
as a numerical property of a field, much like potential energy for example. The ball in the valley
will be used as the main reference point because of its simplicity, but I constantly referred back 
to the other cases given as a source of intuition, to examine properties of Stability that
transcended any one example.

\section{Dynamical Systems}
\subsection{Definition}
  To begin, I believe it is important to clearly define what is meant by a system. In the example of 
the ball in a valley, the system can be said to be defined by the set of points bounded within the valley
in combination with the laws of motion that dictate the ball's path. Thus in this paper, 
a unique dynamic system of dimension $n$, denoted as $\mathcal{S}$,
will refer to a unique domain $\mathcal{D} \in \mathbb{R}^n$ in conjunction with an associated 
Evolution Function $f_s(\B{x}) : \mathcal{D} \to \mathbb{R}^n$ that gives the `trajectory' of a particle 
when given its current `position'. Inside of the domain $\mathcal{D}$, a point can be described using the vector $\B{x} \in \mathcal{D}$. 
Again referring to the initial example, the area encompassing the valley
can be thought of as the system's domain $\mathcal{D}$, while the Evolution Function can be thought of as the
laws of motion. A more formal description will be given in the following paragraph. 

\subsection{Path Notation}

After considering the initial premise, I realized that these conditions meant that, knowing 
the initial position $\B{x}_0$ of a particle at time 0, then using the aforementioned Evolution Function,
we are able to derive the position of the particle at all future times. Therefore, I created a notation 
of Paths to describe this idea. For the particle beginning at the point $\B{x}_0$ at time 0, the corresponding 
Path Function $P_{x_0}(t)$ is a function from $\left[0, \infty \right) \to \mathcal{D}$ such that for any time 
$t_0 \in \left[0, \infty \right)$, the position of the particle at that instant is given by $P_{x_0}(t_0)$. For example,
by definition, $P_{x_0}(0)$ will always give the particle's initial position - or in other words, $P_{x_0}(0) = \B{x}_0$.

\subsection{Evolution Function}

The time derivative of the Path Function can be used to formally define a system's Evolution Function. 
By convention, the time derivative $\frac{d}{dt} \left(P_{x_0}(t)\right)$ will be notated as $\dot{P}_{x_0}(t)$, 
with the single dot above the function indicating a first derivative. Earlier, it was said that
the Evolution Function will return a particle's trajectory in time given its current position. Therefore,
the Evolution Function $f_s(\B{x})$ of the system $\mathcal{S}$ is defined as satisfying 
\begin{align}
  f_s(P_{x_0}(t)) = \dot{P}_{x_0}(t) \label{eq:System Equation} 
\end{align}
meaning that the change with respect to time of a particle's position is given by the Evolution Function evaluated at the 
particle's current location.

% Succinct Definition :(
% We begin this section by defining the system $\mathcal{S}$ as $f_s(\B{x}) : \mathcal{D} \to \mathbb{R}^n$, where
% $\mathcal{D} \subseteq \mathbb{R}^n$. We use $\B{x}_0$ to represent a general vector in $\mathcal{D}$, and define 
% $P_{x_0}(t) : \left[0, \infty \right) \to \mathcal{D}$ to be a path function with the following properties.
% $P_{x_0}(0) = \B{x}_0$ and $\frac{d}{dt} \left(P_{x_0}(t)\right)  = \dot{P}_{x_0}(t) = f_s(P_{x_0}(t)).$

\section{Lypaunov Point Stability}
In his original article from 1892, \cite{Lyapunov:1992} 
layed out a definition for stability in a Dynamical System which is mathematically equivilant to the following:
\begin{definition}[Lyapunov Stability]
  \label{Lyapunov Stability}
  Let there exist a system $\mathcal{S}$ as previously defined. Within this system, suppose
  we claim there is an equilibrium point $\B{x}_e \in \mathcal{D}$.
  Then the point $\B{x}_e$ is stable iff, for every positive real number $\epsilon > 0$, we 
  can find $\delta > 0$ such that, for any point $\B{x}_0$ where $\norm{\B{x}_e - \B{x}_0} < \delta$,
  the path $P_{x_0}(t)$ satisfies the inequality $\norm{\B{x}_e - P_{x_0}(t)} < \epsilon$
  for all $t \in \left[0, \infty \right)$.
  
\end{definition}

In essence, what this definition proposes is that a point is stable if a particle that
starts `close' to it will always remain `close' to the initial point, even as time goes 
to infinity. I found this formalization very compelling for two main reasons. First of all,
it captures the core of what the intuitive understanding of stability.
Secondly, Lyapunov's definition allows for stability to be treated in a much more rigorous and analytical
way, as the conditions it sets out can definitively proven to be true or false using strictly mathematical tools.
Not only that, but because of its usage of the epsilon-delta formalization at the core of
calculus, the further developments that I made followed very naturally, and I was able to use familiar methods of attack
despite the novelty of the problem.



 

% \begin{remark}
%   \label{OpenBall}
%   Intuitively, a system is considered stable at the point $\B{x}_e$ if, for every open ball $B_\epsilon$
%   around $\B{x}_e$ of size $\epsilon$, there exists another open ball $B_\delta$ of size $\delta$ such
%   that all paths that start in $B_\delta$ will stay within $B_\epsilon$.
% \end{remark}

\section{Path Stability}
Despite the ingenuity of Lyapunov's definition, there was still one major difference between my conception of stability and Lyapunov's.
While Lyapunov treated stability in terms of the behavior of local points, I wanted to 
describe stability in terms of local \textbf{\textit{paths}}. Because I really liked
methods Lyapunov used in his approach, the first thing I did was to take Lyapunov's original 
conclusions and rephrase the formalization to relate to the behavior of paths. That 
is how I came to my definition of Path Stability, as follows:

\begin{definition}[Path Stability]
  \label{def:Path Stability}
  Within the system $\mathcal{S}$, suppose there exists the path $P_{x_0}$ which starts at $\B{x}_e$.
  This path is Unbounded Stable iff, for every positive real number $\epsilon > 0$, we can find $\delta > 0$ such that,
  for any point $\B{x}_0$ where $\norm{\B{x}_e - \B{x}_0} < \delta$,
  the paths $P_{x_e}$ and $P_{x_0}$ satisfy the inequality 
  $\norm{P_{x_e}(t) - P_{x_0}(t)} < \epsilon$
  for all $t \in \left[0, \infty \right)$.
\end{definition}

The first thing to remark from definition \ref{def:Path Stability} is the similarities it shares
with the original definition \ref{Lyapunov Stability}. In fact, the two definitions are 
mathematically identical except for, where the stable \textbf{point} $\B{x}_e$ is used
by Lyapunov, my definition substitutes the stable \textbf{path} $P_{x_e}$. This subtle 
yet powerful difference will be discussed in the following sections. 

However, to conclude the current section, I will draw attention to the obvious corrolary resulting from the 
parallel definitions - that any point which satisfies the conditions of Lyapunov 
Stability must also be the origin of a corresponding path that satisfies Path Stability.
To illustrate this, note that in order for a point to be Lyapunov Stable, it must be 
a point of equilibrium - in other words, its path never moves and is defined as the constant function 
$P_{x_e}(t) = \B{x_e}$ for every time $t$. Therefore, any other nearby path that permenantly stays 
`close' to the \textit{point} $\B{x_e}$ will consequently also stay `close' to the \textit{path} $P_{x_e}(t)$.
This corrolary serves to clearly show that Path Stability is not only fully compatable with Lyapunov's  
definition, but in fact generalizes the scope of the original result.  

% \begin{theorem}
% Let $\B{x}_e$ be a stable point in $\mathcal{D}$. Then for every $\epsilon > 0$,
% there exists $\delta > 0$ such that all paths $P_{x_0}$ where $\norm{\B{x}_0 - \B{x}_e} < \delta$ are
% Unbounded Stable as defined in \ref{Unbounded Stability}.
% \end{theorem}

% \begin{proof}
%   Given $\epsilon$, let $B_\alpha$ be equal to the open ball centered at $\B{x}_e$ of radius $\epsilon/2$.
%   From theorem \ref{Lyapunov Stability}, there exists the open ball $B_\beta$ with radius $\beta$ such that all paths
%   starting in $B_\beta$ remain in $B_\alpha$. By the definition of an open set, each point $\B{x}_k$
%   within an open ball $B$ is itself
%   the center of an open ball of radius $r_k > 0$ contained entirely within $B$. Therefore, for some
%   $\B{x}_0 \in B_\beta$, if $\delta_2 = \beta - \norm{\B{x}_0} > \norm{\B{x}_\beta}$, then
%   $\B{x}_0 + \B{x}_\beta \in B_\beta$ for any $\B{x}_\beta$. Therefore, $\forall t \geq 0$,
%   $P_{x_0}(t) \text{ and } P_{x_0 + x_\beta}(t) \in B_\alpha$. For any two points $\B{x}_1, \B{x}_2 \in B_\alpha$,
%   $\norm{\B{x}_1 - \B{x}_2} < 2\norm{B_\alpha} = 2 \epsilon/2 = \epsilon$. Therefore
%   $\norm{P_{x_0}(t) - P_{x_0 + x_\delta}(t)} < \epsilon$ $\forall t \in \left[0, \infty \right)$.
% \end{proof}

\section{General Conditions For Path Stability}
\subsection{Motivation}
After laying out a rigorous definition for what I had originally conceptualized in Path Stability,
my next goal, and the focus of the rest of this paper, was to derive the general conditions
that were needed to satisfy definition \ref{def:Path Stability}. As has been shown, 
Lyapunov Stable equilibrium points are always Path Stable, but far more interesting to me 
was studying Path Stability for non-equilibrium points of origin. 

In particular, I noted that the path a particle travels can be defined through 
the set of all points that the path passes through. For a given path, any point that lies 
in the system can be definitively shown to either lie on the path or not lie on the path. 
The path itself is a real-valued continuous function from $\mathcal{R}^+$ to $\mathcal{D}$,
which is both not finitely bounded and can be arbitrarily complex, making general considerations
on path functions not limited to special cases exceeding difficult. Meanwhile, the constituent \textit{points}
that comprise the path are well defined vector elements, which are much simpler to study. In addition, 
while the set of points that lie along a given path may be uncountably infinite,
because of the conditions on the system of continuity and differentiability, properties of points along 
a path will also in general be continuous and differentiable, meaning that generalized and meaningful results
can still be drawn about the entire set. For this reason, I directed my focus to finding what \textit{local} 
properties on the individual points along a path must be satisfied in order for the path to be stable.  

\subsection{Time Invariance}
Firstly, I wanted to note one particular corrolary derived from the fact that paths were
composed of a set of points: each point is also the origin of its own corresponding path.
If the path originating at point $\B{x}_0$ arrives at point $\B{x}_1$ at the time $t_0$,
then the two will trace identical trajectories from that point onward; in other words,
$$P_{\B{x}_0}(t_0 + s) = P_{\B{x}_1}(t_1 + s)$$

The following is the rigorous proof of this idea, which I refer to as Time Invariance. 

\begin{theorem}[Time Invariance]
  \label{Time Invariance}
  For any two paths $P_{\B{x}_0}$ and $P_{\B{x}_1}$, if there exists $t_0, t_1 \geq 0$ such that
  $P_{\B{x}_0}(t_0) = P_{\B{x}_1}(t_1)$, then $P_{\B{x}_0}(t_0 + s) = P_{\B{x}_1}(t_1 + s)$ for all $s \geq 0$.
\end{theorem}

\begin{proof}
  From the Taylor series expansion of a function $f(t)$ as found in \cite{AdvancedCalculus},
  \begin{align}
    P_{\B{x}_0}(t_0 + s) &= P_{\B{x}_0}(t_0) + \sum_{n=1}^{\infty} \frac{s^n}{n!} P_{\B{x}_0}^{(n)}(t_0) \nonumber\\
    & = P_{\B{x}_0}(t_0) + \sum_{n=1}^{\infty} \frac{s^n}{n!} \left( \left. \frac{d}{dt}P_{\B{x}_0}^{(n-1)}(t) \right\rvert_{t=t_0} \right) \nonumber\\
    & = P_{\B{x}_0}(t_0) + \sum_{n=1}^{\infty} \frac{s^n}{n!} \left( \B{f}_s^{(n-1)} \left( P_{\B{x}_0}(t_0) \right) \right) \nonumber\\
    & = P_{\B{x}_1}(t_1) + \sum_{n=1}^{\infty} \frac{s^n}{n!} \left( \B{f}_s^{(n-1)} \left( P_{\B{x}_1}(t_1) \right) \right) \nonumber\\
    & = P_{\B{x}_1}(t_1 + s) \nonumber
  \end{align}
\end{proof}

\subsection{Local Attractiveness}

The criteria I layed out for Path Stability in definition \ref{def:Path Stability} can 
be stated as: if a given path $P_{x_0}$ is stable, then any other path that starts `nearby'
$P_{x_0}$ must remain `nearby' for time $t \to \infty$.

I considered what this meant for each individual point. If, at each point along the path,
all `nearby' paths would be guaranteed to remain `nearby' for an arbitrarily small period of time,
then at no point along the entire path could a `nearby' path diverge. Intuitively, this might
be obvious, but to specify precisely what is meant by `nearby' and `diverge', I developed the concept 
of Local Attractiveness.  

\begin{definition}[Local Attractiveness]
  \label{def:Local Path Attractiveness}
  A point $\B{x}_0 \in \mathcal{D}$ is said to be locally attractive if,
  for every $\epsilon > 0$, there exists $\delta \in \left(0, \epsilon\right]$ such that
  for every $\B{x}_\delta$ where $\norm{\B{x}_\delta} < \delta$,
  there exists $\sigma > 0$ for which the following is true
  for all $\Delta t \in \left[0, \sigma \right)$.
  \begin{align}
  \norm{P_{x_0}(0) - P_{\left(x_0 + x_\delta\right)}(0)} \ge \norm{P_{x_0}(\Delta t) - P_{\left(x_0 + x_\delta\right)}(\Delta t)}.
  \end{align}  
\end{definition}

Note the similarities between this definition and the epsilon-delta definition of the limit, which
arose because I tried to use the concept of the limit to `squeeze' all 
`nearby' points to converge to the stable path.

\begin{theorem}
  \label{Attractiveness Stability}
  For some path $P_{\B{x}_0}$, if every point on the path denoted by
  $\B{x}_t = P_{\B{x}_0}(t)$, where $t \in \left[0, \infty \right)$, is locally attractive,
  then the path $P_{\B{x}_0}$ is Path Stable as defined in \ref{def:Path Stability}.
\end{theorem}

% ALSO NEED TO INCLUDE TIME INVARIANCE FML
\begin{proof}
  Let $\tau > 0$ be an arbitrarily large length of time.
  For a given $\epsilon > 0$ and some $t \in [0, \tau]$, let $\delta_t \in \left(0,\epsilon\right]$ be equal to the
  corresponding $\delta$ value given in definition \ref{def:Local Path Attractiveness}.
  We define $\delta_{min} = min\{ \delta_t: 0 \leq t \leq \tau\}$, and let $P_{\B{x}_\phi}(t)$ be any path such that $\norm{P_{\B{x}_\phi}(0)-P_{\B{x}_0}(0)} < \delta_{min}$.

  In the following, we use proof by contradiction to prove that, for all $\B{x}_\phi$ where $\norm{\B{x}_\phi - \B{x}_0} < \delta_{min}$ and $t \in [0, \tau]$, $\norm{P_{\B{x}_\phi}(t)-P_{\B{x}_0}(t)} \leq \norm{\B{x}_\phi - \B{x}_0}$.

  We let $D(t) = \norm{P_{\B{x}_\phi}(t)-P_{\B{x}_0}(t)}$, and assume that there exists
  some $t_0 \in [0, \tau]$ such that $D(t_0) > D(0) = \norm{\B{x}_\phi - \B{x}_0}$.
  By definition of a path function, $P_{\B{x}_\phi}(t)$ and $P_{\B{x}_0}(t)$ are continuous on domain
  $\left[0, \infty\right)$ and the norm $\norm{\B{x}}$ is continuous on domain $\mathcal{D}$.
  Therefore, $D(t)$ is also continuous on $t \in \left[0, \infty\right)$, and there must exist some $t_1 \in \left[0, t_0\right)$
  such that $D(t_1) = D(0)$ and,
  \begin{align}
  D(t_k) > D(t_1) = D(0) = \norm{\B{x}_\phi - \B{x}_0}, ~~~~~~~ &\forall ~t_k \in \left(t_1, t_0\right).  \label{contradiction}
  \end{align}

  We let $\B{x}_\alpha = P_{\B{x}_0}(t_1)$ and $\B{x}_\beta = P_{\B{x}_\phi}(t_1)$. Then we have,
  \begin{align}
  D(t_1) &= \norm{P_{\B{x}_\phi}(t_1)-P_{\B{x}_0}(t_1)} \nonumber\\
  &= \norm{\B{x}_\beta - \B{x}_\alpha} \nonumber\\
  &= D(0) \nonumber\\
  & = \norm{P_{\B{x}_\phi}(0)-P_{\B{x}_0}(0)} \nonumber\\
  &= \norm{\B{x}_\phi - \B{x}_0} \nonumber\\
  &< \delta_{min} \leq \delta_{t_1},
  \end{align}
  from which there exists some $\sigma$ such that for all $\Delta t \in \left[0, \sigma\right)$, we have,
  \begin{align}
  \norm{P_{\B{x}_\alpha}(0) - P_{\B{x}_\beta}(0)} \ge \norm{P_{\B{x}_\alpha}(\Delta t) - P_{\B{x}_{\beta}}(\Delta t)} \label{attractiveness}
  \end{align}
  by local attractiveness.
      
  Since $P_{\B{x}_\alpha}(0) = \B{x}_\alpha$, we have $P_{\B{x}_0}(t_1 + s) = P_{\B{x}_\alpha}(s)$   for all $s \geq 0$ by theorem \ref{Time Invariance}. Similarly, we have $P_{\B{x}_\phi}(t_1 + s) = P_{\B{x}_\beta}(s)$ for all $s \geq 0$. By setting $s=0$ on the left side 
  of equation \ref{attractiveness} and $s=\Delta t$ on the right side of equation \ref{attractiveness}, we obtain,
  $$
  D(t_1) = \norm{P_{\B{x}_0}(t_1) - P_{\B{x}_\phi}(t_1)} \geq \norm{P_{\B{x}_0}(t_1 + \Delta t) - P_{\B{x}_\phi}(t_1 + \Delta t)},
  $$
  for some $\Delta t \in \left(0, t_0 - t_1\right)$. This is contradictory with equation~\ref{contradiction}.  
  Therefore, $\norm{P_{\B{x}_\phi}(t)-P_{\B{x}_0}(t)} \leq \norm{\B{x}_\phi - \B{x}_0} < \delta_{min}$ for all
  $t \in [0, \tau]$. Since $\tau$ can be an arbitrarily large length of time, we can conclude that $P_{\B{x}_0}(t)$ is bounded stable by denoting
  $\delta=\delta_{min}$ and $\B{x}_\delta=\B{x}_\phi - \B{x}_0$.
\end{proof}

\section{Conditions For Local Attractiveness}
\subsection{Local Distance Derivative}
Having shown that Local Attractiveness directly results in Path Stability, the 
next question to answer is what are the general conditions for a point to be Locally Attractive?
From the definition, it is clear that if all `nearby' paths around a point are getting `closer'
for some duration of time, then that point is locally attractive. This can be denoted analytically as follows.

\begin{theorem}
  \label{theorem:Local Distance}
  For some point $\B{x}_0$, if there exists $\delta > 0$ such that every
  $\B{x}_\alpha$ where $\norm{\B{x}_0 - \B{x}_\alpha} < \delta$ satisfies the inequality
  \begin{align}
    \frac{d}{dt} \norm{P_{\B{x}_0}(0) - P_{\B{x}_\alpha}(0)} < 0 \label{eq:norm-inequality}
  \end{align}
  Then $\B{x}_0$ is locally attractive.
\end{theorem}

\begin{proof}
  From equation \refeq{eq:norm-inequality}, the derivative of the distance between the two paths is less than 0. 
  Therefore, by the Mean Value Theorem from \cite{AdvancedCalculus}
  and the definition of the derivative, there exists $t_1 > 0$ such that for the following is true for all 
  $\Delta t \in \left(0, t_1\right)$.
  \begin{align}
  \norm{P_{\B{x}_\alpha}(\Delta t) - P_{\B{x}_0}(\Delta t)} < \norm{P_{\B{x}_\alpha}(0) - P_{\B{x}_0}(0)}.\label{attractive-condition}
  \end{align}
  By definition~\ref{def:Local Path Attractiveness}, the point $\B{x}_0$ is locally attractive.
\end{proof}


\subsection{Local Dot Product}
Next, what conditions must a point meet such that the derivative of the distance between
it and any other local path is less than 0? To answer this, I considered that the displacement
between any two paths $\Lambda(t) = (P_{\B{x}_\alpha}(\Delta t) - P_{\B{x}_0}(\Delta t))$ is itself
a continuously differentiable vector valued function $\Lambda(t) : \mathbb{R} \to \mathbb{R}^n$. For a general vector,
its magnitude will decrease if another small vector that points in the `opposite' direction is added to it.
Mathematically, the `opposite' direction can be denoted by the inner product $\hat{v} \bigcdot~ \hat{w} < 0$.
Therefore:



\begin{theorem}
  \label{theorem:Dot Product Distance}
  Given the continuous and differentiable vector valued function $\Lambda(t) : \mathbb{R} \to \mathbb{R}^n$, 
  $$ \Lambda(t) \bigcdot~ \dot{\Lambda(t)} < 0 \Longrightarrow \frac{d}{dt}  \norm{\Lambda(t)}  < 0$$
  % For some point $\B{x}_0$, if there exists $\delta > 0$ such that every
  % $\B{x}_\alpha$ where $\norm{\B{x}_0 - \B{x}_\alpha} < \delta$ satisfies the inequality
  % $$(\B{x}_0 - \B{x}_\alpha) \bigcdot~ \left(\dot{P}_{\B{x}_0}(0) - \dot{P}_{\B{x}_\alpha}(0)\right) < 0$$
  % Then $\B{x}_0$ is locally attractive.
\end{theorem}

\begin{proof}
  Let $\Lambda(t): \mathbb{R} \to \mathbb{R}^n$ be a continuous and differentiable vector valued
  function such that
  \begin{align}
    \Lambda(t) \bigcdot~ \dot{\Lambda(t)} < 0 \label{eq:Lambda Dot}
  \end{align}
  Let the operator $\norm{\Lambda(t)}$ be the 2-norm of $\Lambda(t)$.

  For any time $t$, we re-write $\Lambda(t)$ as $\left( \Lambda_1(t), \Lambda_2(t)..., \Lambda_n(t) \right)$
  where $\Lambda_i(t) : \mathbb{R} \to \mathbb{R}$ for $i = 1, 2, \cdots, n$. By definition of the norm in $\mathbb{R}^n$,
  \begin{align}
    \norm{\Lambda}^2 &= \sum_{i=1}^n \Lambda_i(t)^2 & \Longrightarrow \nonumber\\
    \frac{d}{dt} \norm{\Lambda}^2 &= \frac{d}{dt} \left(\sum_{i=1}^n \Lambda_i(t)^2\right)  & \Longrightarrow \nonumber\\
    2\norm{\Lambda} \norm{\Lambda}^\prime &= 2\sum_{i=1}^n \Lambda_i(t)\dot{\Lambda}_i(t)  & \Longrightarrow \nonumber\\
    \norm{\Lambda}^\prime &= \frac{\Lambda(t)  \bigcdot~ \dot{\Lambda}(t)}{\norm{\Lambda}} 
  \end{align}
  From equation \refeq{eq:Lambda Dot}, $\Lambda(t) \bigcdot~ \dot{\Lambda(t)} < 0$. From the definition of the norm,
  $\norm{\Lambda} > 0$. Therefore, 
  \begin{align}
    \frac{d}{dt} \norm{\Lambda} &= \norm{\Lambda}^\prime = \frac{\Lambda(t)  \bigcdot~ \dot{\Lambda}(t)}{\norm{\Lambda}} < 0 \label{Distance Derivative}
  \end{align}
\end{proof}

From theorem \ref{theorem:Local Distance}, the path functions $P_{\B{x}_\alpha}(t)$ and $P_{\B{x}_0}(t)$ are both
continuously differentiable by definition. Therefore, $\Lambda(t) = (P_{\B{x}_\alpha}(\Delta t) - P_{\B{x}_0}(\Delta t))$ is 
also continuously differentiable.
Therefore, by theorems \ref{theorem:Local Distance} and \ref{theorem:Dot Product Distance}, we have:

\begin{theorem}
  \label{Local Dot Product}
  For some point $\B{x}_0$, if there exists $\delta > 0$ such that every
  $\B{x}_\alpha$ where $\norm{\B{x}_0 - \B{x}_\alpha} < \delta$ satisfies the inequality
  \begin{align}
    ({P}_{\B{x}_0}(0) - {P}_{\B{x}_\alpha}(0)) \bigcdot~ \left(\dot{P}_{\B{x}_0}(0) - \dot{P}_{\B{x}_\alpha}(0)\right) < 0 \label{eq:Dot Inequality}
  \end{align}
  Then $\B{x}_0$ is locally attractive.
\end{theorem}


\section{Linearization}
Finally, we will evaluate what conditions are sufficient to that the aforementioned local dot product
is always negative. When dealing with differentials under an arbitrarily small limit, as is the current case,
it is often useful to linearize the function's derivative, which is analogous to taking the slope of a curve.
To linearize a vector function, we use the Jacobian operator, as defined in \cite{AdvancedCalculus}.

\begin{definition}[Jacobian]
  \label{Jacobian Approximation}
  Given $\B{x}_0$ and some unit vector $\B{x}_\delta$ for which $\norm{\B{x}_\delta} = 1$,
  the Jacobian matrix of the vector differentiable vector-valued function $\B{f}_s : \mathbb{R} \to \mathbb{R}^n$ 
  evaluated at point $\B{x}_0$ is the linear transformation 
  from $\mathbb{R}^n \to \mathbb{R}^n$
  denoted by $\B{J}_{\B{f}_s}[\B{x}_0]$ and given in the following equation.
  \begin{align}
  \lim_{c \to 0} \frac{f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)}{c} = \B{J}_{\B{f}_s}[\B{x}_0](\B{x}_\delta). \label{jacobian}
  \end{align}
\end{definition}

\begin{theorem}
  \label{theorem:Jacobian Local Attractive}
  For some point $\B{x}_0$, let $\B{J}_{\B{f}_s}[\B{x}_0]$ be the Jacobian transformation at that point. 
  If, all unit vectors $\left\{\B{x}_\delta \in \mathbb{R}^n : \norm{\B{x}_\delta} = 1\right\}$ satisfy the inequality
  \begin{align}
    \B{x}_\delta \bigcdot~ \B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta < 0 \label{eq:JacobianDot}
  \end{align}
  Then the point $\B{x}_0$ is Locally Attractive.
\end{theorem}

\begin{proof}
  From the definition of the dot product, it is known to be continuous on $\mathbb{R}^n$.
  Therefore, for any two vectors $\B{x}, \B{y} \in \mathbb{R}^n$ where $\B{x} \bigcdot~ \B{y} < 0$, 
  by definition of continuity, 
  there exists some $\epsilon_1 > 0$ such that all $\delta\B{y} \in \mathbb{R}^n$ where
  $\norm{\delta\B{y}} < \epsilon_1$ also satisfy the inequality
  \begin{align}
     \B{x}  \bigcdot~ (\B{y} + \delta\B{y}) < 0.  \label{eq:1}
  \end{align}

  
  From equation \ref{jacobian}, we know that for any $\epsilon_1 > 0$, we can find $\delta_1 > 0$
  such that for scalar $c$,
  \begin{align}
    \abs{c} \in (0, \delta_1) \implies \norm{\frac{f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)}{c} - \B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta} < \epsilon_1 
  \end{align}
  Therefore, knowing $\B{x}_\delta \bigcdot~ \B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta < 0$,
  we can substitute in \refeq{eq:1} $\B{x}$ for $\B{x}_\delta$, $\B{y}$ for $\B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta$
  and $\delta\B{y}$ for $(\frac{f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)}{c} - \B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta)$, giving

  \begin{align}
    \B{x}_\delta \bigcdot~ \left(\B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta + \frac{f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)}{c} - \B{J}_{\B{f}_s}[\B{x}_0] \B{x}_\delta \right) &< 0 \Longrightarrow \nonumber\\
    \B{x}_\delta \bigcdot~ \left(\frac{f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)}{c} \right) &< 0 \Longrightarrow \nonumber \\
    c \B{x}_\delta \bigcdot~ \left({f_s(\B{x}_0 + c \B{x}_\delta) - f_s(\B{x}_0)} \right) &< 0
  \end{align}
  Remembering $f_s(P_{x_0}(t)) = \dot{P}_{x_0}(t)$ from \refeq{eq:System Equation}, and denoting $\B{x}_\alpha = \B{x}_0 + c \B{x}_\delta$ 
  \begin{align}
    (P_{\B{x}_\alpha}(0) - P_{\B{x}_0}(0)) \bigcdot~ \left(\dot{P}_{\B{x}_\alpha}(0) - \dot{P}_{\B{x}_0}(0)\right) < 0
  \end{align}
  Which, by theorem \ref{Local Dot Product}, proves $\B{x}_0$ is locally attractive.
\end{proof}

Finally, we note that linear transforms that satisfy equation \refeq{eq:JacobianDot} are called
Negative Definite, a well studied property within linear algebra.
\begin{definition}[Negative Definite]
  \label{Negative Definite}
  From \cite{LinearAlgebra}, a linear transformation denoted by
  $\mathscr{T}(\B{x}) \in \mathcal{L}_{n,n}$ is said to be Negative Definite
  if, for each non-zero vector $\B{x} \in \mathbb{R}^n$, the Quadratic Form $\B{x}^T \mathscr{T}\B{x} < 0$.
\end{definition}

Therefore, by combining theorems \ref{Attractiveness Stability}, \ref{theorem:Jacobian Local Attractive}, and definition \ref{Negative Definite},
we obtain the final result to answer the original research question:

\begin{theorem} 
  \label{Negative Definite Path Stability}
  For a given path $P_{\B{x}_0}(t)$, if for each point $\B{x}_t = P_{\B{x}_0}(t)$,
  the linear transformation led by $\B{J}_{\B{f}_s}[\B{x}_t]$
  is Negative Definite, then $P_{\B{x}_0}(t)$ is Path Stable.
\end{theorem}
QUOD ERAT DEMONSTRANDUM.

\section{Conclusion}
This paper has explored the idea of stability and its standard mathematical 
representation. Taking Lyapunov's original idea, we have used the same
epsilon-delta formalization to expand the concept to include entire paths.
To determine if a path meets this definition of stability, it has been
shown that if the set of all points on the path meet the criteria for 
local attractiveness, as defined in the paper, then the path overall must 
also be stable. In addition, by showing that points where the Quadratic Form
of the Jacobian is Negative Determinant are always locally attractive, the paper
presents a direct method of calculating the overall stability of a path, and 
provides areas where this concept can be developed even further, most namely into
the complex vector space, as well as into stochastic or time-variant systems. 


\pagebreak
\medskip

\addcontentsline{toc}{section}{References}
\bibliographystyle{apalike}
\bibliography{references}

\end{document}